{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.utils import np_utils, print_summary\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Project 1 data.csv')\n",
    "dataset = np.array(data)\n",
    "np.random.shuffle(dataset)\n",
    "X = dataset\n",
    "Y = dataset\n",
    "X = X[:, 0:1024]\n",
    "Y = Y[:, 1024]\n",
    "\n",
    "#Split to train/test\n",
    "X_train = X[0:70000, :]\n",
    "X_train = X_train/255.  #Value of pixels range from 0-255. So change it from 0-1 (Normalisation basically)\n",
    "X_test = X[70000:72001, :]\n",
    "X_test = X_test/255.\n",
    "\n",
    "#Reshape\n",
    "Y = Y.reshape(Y.shape[0], 1)\n",
    "Y_train = Y[0:70000, :]\n",
    "Y_train =Y_train.T\n",
    "Y_test = Y[70000:72001, :]\n",
    "Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples =  70000\n",
      "Number of test examples =  2000\n",
      "X_train shape:  (70000, 1024)\n",
      "Y_train shape:  (1, 70000)\n",
      "X_test shape:  (2000, 1024)\n",
      "Y_test shape:  (1, 2000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples = \", str(X_train.shape[0]))\n",
    "print(\"Number of test examples = \", str(X_test.shape[0]))\n",
    "print(\"X_train shape: \", str(X_train.shape))\n",
    "print(\"Y_train shape: \", str(Y_train.shape))\n",
    "print(\"X_test shape: \", str(X_test.shape))\n",
    "print(\"Y_test shape: \", str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the height and width of images\n",
    "image_x = 32\n",
    "image_y = 32\n",
    "\n",
    "#keras function - basically used for one-hot encoding\n",
    "train_y = np_utils.to_categorical(Y_train)\n",
    "test_y = np_utils.to_categorical(Y_test)\n",
    "\n",
    "#Shape is basically number of rows across number of classes now (Number of digits/alpha to identify)\n",
    "train_y = train_y.reshape(train_y.shape[1], train_y.shape[2])\n",
    "test_y = test_y.reshape(test_y.shape[1], test_y.shape[2])\n",
    "\n",
    "#The 1 here refers to number of channels. If image is Black&White then channels is 1. If RGB, then channels is 3\n",
    "X_train = X_train.reshape(X_train.shape[0], image_x, image_y, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], image_x, image_y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (70000, 32, 32, 1)\n",
      "Y_train shape:  (70000, 37)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: \", str(X_train.shape))\n",
    "print(\"Y_train shape: \", str(train_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_model(image_x, image_y): #ht, width\n",
    "    num_of_classes = 37\n",
    "    #Sequential model. Can use others also\n",
    "    model = Sequential() \n",
    "    \n",
    "    #apply a conv filter. 2D convolution layer (e.g. spatial convolution over images).\n",
    "    #filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "    #kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window.\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5,5), input_shape=(image_x, image_y, 1), activation='relu'))\n",
    "    \n",
    "    #Max pooling operation for spatial data.\n",
    "    #pool_size: integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension\n",
    "    #padding: One of \"valid\" or \"same\" (case-insensitive).\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "    \n",
    "    #Only the first layer in a model needs to know input_size. Others can infer\n",
    "    model.add(Conv2D(filters=64, kernel_size=(5,5), activation='sigmoid'))\n",
    "    model.add(MaxPooling2D(pool_size=(5,5), strides=(5,5), padding='same'))\n",
    "    \n",
    "    #Flattening a tensor means to remove all of the dimensions except for one. This is exactly what the Flatten layer do.\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #Last layer. Used to output a Y value. Softmax classifier\n",
    "    model.add(Dense(num_of_classes, activation='softmax'))\n",
    "    \n",
    "    #Compile defines the loss function, the optimizer and the metrics. That's all\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    filepath = 'model/devanagari_model_refined.h5'\n",
    "    \n",
    "    #The ModelCheckpoint callback class allows you to define where to checkpoint the model weights, how the file should named and under what circumstances to make a checkpoint of the model.\n",
    "    checkpoint1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint1]\n",
    "    \n",
    "    return model, callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 70000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "70000/70000 [==============================] - 81s 1ms/step - loss: 1.3635 - acc: 0.6902 - val_loss: 0.5293 - val_acc: 0.8765\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.87650, saving model to model/devanagari_model_refined.h5\n",
      "CNN Error:  12.350000000000009\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 10, 10, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 37)                9509      \n",
      "=================================================================\n",
      "Total params: 61,605\n",
      "Trainable params: 61,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model, callbacks_list = keras_model(image_x, image_y)\n",
    "# Fit model. Will try to minimise loss. 64 batch_size to perform gradient descent. Then goes to perform for the next 64 bytes\n",
    "# Can use any number of epochs. We'll use 1 now just for testing purposes\n",
    "model.fit(X_train, train_y, validation_data=(X_test, test_y), epochs=1, batch_size=64, callbacks=callbacks_list)\n",
    "scores = model.evaluate(X_test, test_y, verbose=0)\n",
    "print(\"CNN Error: \", (100-scores[1]*100))\n",
    "\n",
    "#Exactly what all functionality we're using\n",
    "print_summary(model)\n",
    "model.save('model/devanagari.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
